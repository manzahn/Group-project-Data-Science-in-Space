{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bf9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c68b41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_folder = \"path/to/your/csv/folder\"  # Change this to your folder path\n",
    "output_file = \"combined_climate_data.csv\"  # Output file name\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\n",
    "    'Bundle ID(s)',  # Needed for deduplication\n",
    "    'Case Filing Year for Action',\n",
    "    'Status',\n",
    "    'Case Categories',\n",
    "    'Geographies'\n",
    "    # Add 'sector' column name here if you identify it\n",
    "]\n",
    "\n",
    "# Initialize list to store dataframes\n",
    "all_data = []\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for csv_file in Path(input_folder).glob(\"*.csv\"):\n",
    "    print(f\"Processing: {csv_file.name}\")\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Add a category column based on the filename (without extension)\n",
    "    category_name = csv_file.stem  # stem gives filename without extension\n",
    "    df['Category'] = category_name\n",
    "    \n",
    "    # Keep only the columns we need\n",
    "    df = df[columns_to_keep + ['Category']]\n",
    "    \n",
    "    # Add to our list\n",
    "    all_data.append(df)\n",
    "    print(f\"  Loaded {len(df)} rows from {csv_file.name}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\nTotal rows before processing: {len(combined_df)}\")\n",
    "\n",
    "# Create a binary indicator for each category\n",
    "combined_df['value'] = 1\n",
    "\n",
    "# Pivot to create category columns\n",
    "category_pivot = combined_df.pivot_table(\n",
    "    index='Bundle ID(s)',\n",
    "    columns='Category',\n",
    "    values='value',\n",
    "    aggfunc='max',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Get the other columns (take first occurrence for each Bundle ID)\n",
    "other_columns = combined_df.drop(columns=['Category', 'value']).drop_duplicates(subset='Bundle ID(s)', keep='first')\n",
    "\n",
    "# Merge everything together\n",
    "final_df = other_columns.merge(category_pivot, on='Bundle ID(s)')\n",
    "\n",
    "print(f\"Total unique cases: {len(final_df)}\")\n",
    "\n",
    "# Save the result\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nSaved to: {output_file}\")\n",
    "\n",
    "# Show which cases appear in multiple categories\n",
    "category_columns = [col for col in final_df.columns if col not in columns_to_keep]\n",
    "final_df['total_categories'] = final_df[category_columns].sum(axis=1)\n",
    "\n",
    "print(\"\\nCases appearing in multiple categories:\")\n",
    "print(final_df[final_df['total_categories'] > 1]['total_categories'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCategory column summary:\")\n",
    "for col in category_columns:\n",
    "    print(f\"  {col}: {final_df[col].sum()} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z194z2jn2xn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geography Statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GEOGRAPHY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count total cases\n",
    "total_cases = len(final_df)\n",
    "print(f\"\\nTotal cases: {total_cases}\")\n",
    "\n",
    "# Analyze Geographies column\n",
    "# Note: Each row might contain multiple geographies separated by delimiters\n",
    "geography_counts = {}\n",
    "\n",
    "for idx, row in final_df.iterrows():\n",
    "    geographies = str(row['Geographies'])\n",
    "    \n",
    "    # Skip if NaN or empty\n",
    "    if pd.isna(row['Geographies']) or geographies == 'nan':\n",
    "        geography_counts['Unknown/Not Specified'] = geography_counts.get('Unknown/Not Specified', 0) + 1\n",
    "    else:\n",
    "        # Split by common delimiters (adjust based on your data format)\n",
    "        # Common delimiters: semicolon, comma, pipe\n",
    "        import re\n",
    "        geo_list = re.split(r'[;,|]', geographies)\n",
    "        \n",
    "        for geo in geo_list:\n",
    "            geo = geo.strip()\n",
    "            if geo:\n",
    "                geography_counts[geo] = geography_counts.get(geo, 0) + 1\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "geography_stats = pd.DataFrame([\n",
    "    {'Geography': geo, 'Count': count, 'Percentage': (count/total_cases)*100}\n",
    "    for geo, count in sorted(geography_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "])\n",
    "\n",
    "print(\"\\nGeography Distribution:\")\n",
    "print(geography_stats.to_string(index=False))\n",
    "\n",
    "print(f\"\\nTotal unique geographies: {len(geography_counts)}\")\n",
    "\n",
    "# Optional: Create a simple bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show top 15 geographies\n",
    "top_n = 15\n",
    "top_geographies = geography_stats.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_geographies)), top_geographies['Count'])\n",
    "plt.yticks(range(len(top_geographies)), top_geographies['Geography'])\n",
    "plt.xlabel('Number of Cases')\n",
    "plt.ylabel('Geography')\n",
    "plt.title(f'Top {top_n} Geographies by Case Count')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save statistics to CSV\n",
    "geography_stats.to_csv('geography_statistics.csv', index=False)\n",
    "print(\"\\nGeography statistics saved to: geography_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff460f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographies\n",
      "USA;US-DC    253\n",
      "USA;US-CA    135\n",
      "BRA;BR-PA     73\n",
      "BRA           72\n",
      "USA;US-NY     65\n",
      "XAB           58\n",
      "USA           53\n",
      "GBR           48\n",
      "USA;US-AK     41\n",
      "BRA;BR-AM     40\n",
      "USA;US-WA     36\n",
      "USA;US-OR     36\n",
      "NZL           27\n",
      "USA;US-MT     24\n",
      "AUS           23\n",
      "USA;US-MA     23\n",
      "USA;US-TX     22\n",
      "USA;US-LA     20\n",
      "USA;US-MD     19\n",
      "USA;US-AZ     17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load  CSV\n",
    "df = pd.read_csv('combined_climate_data.csv')\n",
    "\n",
    "# Show top 10 most common values in the 'Geographies' column\n",
    "print(df['Geographies'].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fc73f",
   "metadata": {},
   "source": [
    "the most listed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ac144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gds25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
