{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RQ2: Do Land Deals Predict Climate Litigation and ISDS Cases? (MINIMAL)\n\n**Research Question:** Do countries with large-scale land acquisitions experience more climate litigation and investment disputes?\n\n**Hypothesis:** Land grabbing → environmental conflict → both climate cases AND investor disputes\n\n**Analysis:**\n- Model A: Land Deals → Climate Cases\n- Model B: Land Deals → ISDS Cases (ALL SECTORS - revised from Ag/Mining only)\n\n**Unit of Analysis:** Country-level (aggregated data)\n\n---\n\n## VERSION: MINIMAL PREDICTOR SET (5 VARIABLES)\n\n**This is the MINIMAL version maximizing sample size.**\n\nFor the 7-predictor version, see: `RQ2_Land_Deals_to_Litigation_Analysis_REVISED.ipynb`\n\n---\n\n## MINIMAL PREDICTOR SET:\n\n**Variables Removed** (poor coverage):\n- ❌ Rule_of_Law_Score_Pct (46.6% coverage)\n- ❌ Avg_Deal_Size (multicollinear)\n- ❌ Prop_Agriculture (52% missing in Model A, 34% in Model B)\n- ❌ Literacy_Rate_Pct (26% missing in Model A, 10% in Model B)\n\n**Variables Kept** (5 core predictors):\n1. ✅ Total_Deal_Size (hectares)\n2. ✅ Num_Deals (count)\n3. ✅ Corruption_Score (governance)\n4. ✅ Press_Freedom_Score (governance)\n5. ✅ log_Population (control for country size)\n\n**Rationale:**\n- Maximizes statistical power (sample size)\n- Keeps core theoretical variables (land deals + governance)\n- Controls for country size\n- Reduces multicollinearity\n\n**Expected Sample Sizes:**\n- Model A: ~50 countries (was 23 with 7 predictors)\n- Model B: ~113 countries (was 72 with 7 predictors)\n- Observations per predictor: 10.0 and 22.6 respectively ✓"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: DATA AGGREGATION TO COUNTRY LEVEL\n",
    "\n",
    "Aggregate all three datasets to country level and merge with governance indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load datasets\nprint(\"Loading datasets...\")\nlandmatrix = pd.read_csv('Processed_Datasets/landmatrix_final_analytical_dataset.csv')\nisds = pd.read_csv('Processed_Datasets/ISDS_processed_dataset_final.csv')\nclimate = pd.read_csv('Processed_Datasets/climate_litigation_final_unified.csv')\ngovernance = pd.read_csv('indipendent_variables/combined_independent_variables.csv')\npopulation = pd.read_csv('indipendent_variables/WorldPopulationByCountry.csv',\n                         encoding='utf-8-sig', index_col=False)\n\nprint(f\"Land Matrix: {len(landmatrix)} deals\")\nprint(f\"ISDS: {len(isds)} cases\")\nprint(f\"Climate: {len(climate)} cases\")\nprint(f\"Governance: {len(governance)} countries\")\nprint(f\"Population: {len(population)} countries\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Land Matrix to country level\n",
    "print(\"\\nAggregating Land Matrix by country...\")\n",
    "\n",
    "# Filter valid deals (size > 0)\n",
    "lm_valid = landmatrix[landmatrix['Deal size'] > 0].copy()\n",
    "\n",
    "# Get primary sector\n",
    "def get_primary_sector(nace_str):\n",
    "    if pd.isna(nace_str):\n",
    "        return 'Unknown'\n",
    "    return nace_str.split(' ')[0]\n",
    "\n",
    "lm_valid['primary_sector'] = lm_valid['nace_sector'].apply(get_primary_sector)\n",
    "\n",
    "# Aggregate by country\n",
    "lm_country = lm_valid.groupby('Target country_iso3').agg({\n",
    "    'Deal size': ['sum', 'mean', 'count'],\n",
    "    'primary_sector': lambda x: (x == 'A').sum() / len(x),  # Proportion Agriculture\n",
    "}).reset_index()\n",
    "\n",
    "lm_country.columns = ['ISO3', 'Total_Deal_Size', 'Avg_Deal_Size', 'Num_Deals', 'Prop_Agriculture']\n",
    "\n",
    "# Add proportion mining\n",
    "lm_country['Prop_Mining'] = lm_valid.groupby('Target country_iso3').apply(\n",
    "    lambda x: (x['primary_sector'] == 'B').sum() / len(x)\n",
    ").values\n",
    "\n",
    "print(f\"Countries with land deals: {len(lm_country)}\")\n",
    "print(lm_country.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Climate Litigation to country level\n",
    "print(\"\\nAggregating Climate Litigation by country...\")\n",
    "\n",
    "# Extract main country (first in Geographies)\n",
    "climate['main_country'] = climate['Geographies'].str.split(';').str[0].str.strip()\n",
    "climate['iso3_clean'] = climate['main_country'].str[:3]  # Extract ISO3\n",
    "\n",
    "# Get primary sector for climate\n",
    "def get_primary_sector_climate(row):\n",
    "    for sector in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'O']:\n",
    "        if row.get(f'sector_{sector}', 0) == 1:\n",
    "            return sector\n",
    "    return 'Unknown'\n",
    "\n",
    "climate['primary_sector'] = climate.apply(get_primary_sector_climate, axis=1)\n",
    "\n",
    "# Aggregate by country\n",
    "climate_country = climate.groupby('iso3_clean').agg({\n",
    "    'Case URL': 'count',  # Total cases\n",
    "    'Status': lambda x: (x.isin(['Decided', 'Decided '])).sum() / len(x),  # Proportion decided\n",
    "    'primary_sector': lambda x: (x == 'A').sum() / len(x),  # Proportion Agriculture\n",
    "}).reset_index()\n",
    "\n",
    "climate_country.columns = ['ISO3', 'Climate_Cases', 'Prop_Climate_Decided', 'Prop_Climate_Agriculture']\n",
    "\n",
    "print(f\"Countries with climate cases: {len(climate_country)}\")\n",
    "print(climate_country.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate ISDS to country level\n",
    "print(\"\\nAggregating ISDS by country...\")\n",
    "\n",
    "# Get primary sector for ISDS\n",
    "def get_primary_sector_isds(row):\n",
    "    for sector in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N']:\n",
    "        if row.get(f'sector_{sector}', 0) == 1:\n",
    "            return sector\n",
    "    return 'Unknown'\n",
    "\n",
    "isds['primary_sector'] = isds.apply(get_primary_sector_isds, axis=1)\n",
    "\n",
    "# Filter Agriculture & Mining cases\n",
    "isds['is_ag_mining'] = isds['primary_sector'].isin(['A', 'B'])\n",
    "\n",
    "# Aggregate by respondent country\n",
    "isds_country = isds.groupby('respondent_state_iso3').agg({\n",
    "    'amount_claimed_musd': ['count', 'mean', 'sum'],\n",
    "    'is_ag_mining': 'sum',  # Count of Ag/Mining cases\n",
    "}).reset_index()\n",
    "\n",
    "isds_country.columns = ['ISO3', 'ISDS_Cases', 'Avg_Amount_Claimed', 'Total_Amount_Claimed', 'ISDS_Cases_Ag_Mining']\n",
    "\n",
    "print(f\"Countries with ISDS cases: {len(isds_country)}\")\n",
    "print(isds_country.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge all datasets\nprint(\"\\nMerging all datasets...\")\n\n# Start with governance (has all countries)\nmerged = governance.copy()\n\n# Merge land matrix\nmerged = merged.merge(lm_country, on='ISO3', how='left')\n\n# Merge climate\nmerged = merged.merge(climate_country, on='ISO3', how='left')\n\n# Merge ISDS\nmerged = merged.merge(isds_country, on='ISO3', how='left')\n\n# Clean and merge population data\npopulation_clean = population.copy()\npopulation_clean.columns = population_clean.columns.str.strip()  # Remove whitespace\npopulation_clean['ISO3'] = population_clean['Iso code'].astype(str).str.strip()  # Convert to string first\npopulation_clean = population_clean[['ISO3', 'pop2025']].rename(columns={'pop2025': 'Population'})\n\nmerged = merged.merge(population_clean, on='ISO3', how='left')\n\n# Create log-transformed population (handle NaN and zeros)\nmerged['log_Population'] = np.log(merged['Population'].replace(0, np.nan))\n\n# Fill NaN in count variables with 0 (no cases = 0)\ncount_cols = ['Num_Deals', 'Climate_Cases', 'ISDS_Cases', 'ISDS_Cases_Ag_Mining']\nmerged[count_cols] = merged[count_cols].fillna(0)\n\n# Fill NaN in size variables with 0\nsize_cols = ['Total_Deal_Size', 'Avg_Deal_Size', 'Total_Amount_Claimed', 'Avg_Amount_Claimed']\nmerged[size_cols] = merged[size_cols].fillna(0)\n\nprint(f\"\\nMerged dataset: {len(merged)} countries\")\nprint(f\"Countries with Population data: {merged['Population'].notna().sum()}\")\nprint(f\"Countries with log_Population data: {merged['log_Population'].notna().sum()}\")\nprint(f\"\\n--- Sample Sizes by Dataset ---\")\nprint(f\"Countries with land deals: {(merged['Num_Deals'] > 0).sum()}\")\nprint(f\"Countries with climate cases: {(merged['Climate_Cases'] > 0).sum()}\")\nprint(f\"Countries with ISDS cases (all sectors): {(merged['ISDS_Cases'] > 0).sum()}\")\nprint(f\"Countries with ISDS Ag/Mining cases: {(merged['ISDS_Cases_Ag_Mining'] > 0).sum()}\")\nprint(f\"\\nCountries with ALL THREE datasets: {((merged['Num_Deals'] > 0) & (merged['Climate_Cases'] > 0) & (merged['ISDS_Cases'] > 0)).sum()}\")\n\n# Save merged dataset\nmerged.to_csv('results/minimal/merged_country_level_data_minimal.csv', index=False)\nprint(\"\\nSaved: results/minimal/merged_country_level_data_minimal.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL A: Land Deals → Climate Cases\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: DATA INSPECTION (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter countries with climate cases\n",
    "df_a = merged[merged['Climate_Cases'] > 0].copy()\n",
    "\n",
    "print(f\"Countries with climate cases: {len(df_a)}\")\n",
    "print(f\"\\nShape: {df_a.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df_a.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(df_a.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df_a.isnull().sum()[df_a.isnull().sum() > 0])\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "cols_of_interest = ['Climate_Cases', 'Total_Deal_Size', 'Num_Deals', 'Corruption_Score', 'Rule_of_Law_Score_Pct']\n",
    "print(df_a[cols_of_interest].describe())\n",
    "\n",
    "# Check DV distribution\n",
    "print(\"\\n--- DV Distribution (Climate_Cases) ---\")\n",
    "print(f\"Skewness: {df_a['Climate_Cases'].skew():.2f}\")\n",
    "print(f\"Mean: {df_a['Climate_Cases'].mean():.2f}, Median: {df_a['Climate_Cases'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot DV distribution (before and after log transformation)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Original\naxes[0].hist(df_a['Climate_Cases'], bins=30, edgecolor='black')\naxes[0].set_title('Original: Climate Cases')\naxes[0].set_xlabel('Climate Cases')\naxes[0].set_ylabel('Frequency')\n\n# Log-transformed\ndf_a['Climate_Cases_Log'] = np.log(df_a['Climate_Cases'] + 1)\naxes[1].hist(df_a['Climate_Cases_Log'], bins=30, edgecolor='black', color='orange')\naxes[1].set_title('Log-Transformed: log(Climate Cases + 1)')\naxes[1].set_xlabel('log(Climate Cases + 1)')\naxes[1].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.savefig('results/minimal/modelA_dv_distribution_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"DV will be log-transformed for modeling.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: CORRELATION CHECK (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select numeric predictors (MINIMAL: 5 core variables)\npredictors_a = ['Total_Deal_Size', 'Num_Deals',\n                'Corruption_Score', 'Press_Freedom_Score',\n                'log_Population']\n\n# Check missing data BEFORE dropping\nprint(\"--- Missing Data Analysis (Model A - MINIMAL) ---\")\nprint(f\"Countries with Climate cases: {len(df_a)}\")\nfor pred in predictors_a:\n    missing = df_a[pred].isna().sum()\n    print(f\"  Missing {pred}: {missing} ({missing/len(df_a)*100:.1f}%)\")\n\n# Drop rows with missing predictors\ndf_a_clean = df_a.dropna(subset=predictors_a)\nprint(f\"\\nAfter dropping missing predictors: {len(df_a_clean)} countries\")\nprint(f\"Lost {len(df_a) - len(df_a_clean)} countries due to missing data\")\nprint(f\"Observations per predictor: {len(df_a_clean) / len(predictors_a):.1f}\\n\")\n\n# Correlation matrix\ncorr_matrix = df_a_clean[predictors_a].corr()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Matrix - Model A Predictors (MINIMAL)', fontsize=14)\nplt.tight_layout()\nplt.savefig('results/minimal/modelA_correlation_matrix_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Flag high correlations\nprint(\"\\n--- High Correlations (|r| > 0.7) ---\")\nhigh_corr = np.where(np.abs(corr_matrix) > 0.7)\nhigh_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y]) \n                   for x, y in zip(*high_corr) if x != y and x < y]\nfor var1, var2, corr in high_corr_pairs:\n    print(f\"{var1} <-> {var2}: {corr:.3f}\")\n\nif not high_corr_pairs:\n    print(\"No high correlations found.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: PREPARE DATA (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y_a = df_a_clean['Climate_Cases_Log']\n",
    "X_a = df_a_clean[predictors_a]\n",
    "\n",
    "print(f\"X shape: {X_a.shape}\")\n",
    "print(f\"y shape: {y_a.shape}\")\n",
    "\n",
    "# Standardize predictors\n",
    "scaler_a = StandardScaler()\n",
    "X_a_scaled = pd.DataFrame(scaler_a.fit_transform(X_a), columns=X_a.columns, index=X_a.index)\n",
    "\n",
    "# Train/test split\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
    "    X_a_scaled, y_a, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train_a)} countries\")\n",
    "print(f\"Test set: {len(X_test_a)} countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: FIT BASELINE MODEL (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS regression\n",
    "model_a = LinearRegression()\n",
    "model_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "# Get coefficients\n",
    "coef_df_a = pd.DataFrame({\n",
    "    'Feature': X_train_a.columns,\n",
    "    'Coefficient': model_a.coef_\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- MODEL A: Land Deals → Climate Cases ---\")\n",
    "print(f\"Intercept: {model_a.intercept_:.4f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coef_df_a.to_string(index=False))\n",
    "\n",
    "# Train R²\n",
    "train_r2_a = model_a.score(X_train_a, y_train_a)\n",
    "print(f\"\\nTrain R²: {train_r2_a:.4f}\")\n",
    "\n",
    "# Calculate VIF (simplified - check correlations)\n",
    "print(\"\\n--- VIF Check (using correlation method) ---\")\n",
    "print(\"Note: VIF > 5 indicates potential multicollinearity\")\n",
    "for col in X_a_scaled.columns:\n",
    "    # Simple VIF approximation: 1 / (1 - R²) from regressing this var on others\n",
    "    X_temp = X_a_scaled.drop(columns=[col])\n",
    "    y_temp = X_a_scaled[col]\n",
    "    lr_temp = LinearRegression()\n",
    "    lr_temp.fit(X_temp, y_temp)\n",
    "    r2_temp = lr_temp.score(X_temp, y_temp)\n",
    "    vif = 1 / (1 - r2_temp) if r2_temp < 0.999 else 999\n",
    "    flag = \" ⚠️\" if vif > 5 else \"\"\n",
    "    print(f\"{col}: {vif:.2f}{flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: TEST SET EVALUATION (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred_a = model_a.predict(X_test_a)\n",
    "\n",
    "# Calculate metrics\n",
    "test_r2_a = r2_score(y_test_a, y_pred_a)\n",
    "test_rmse_a = np.sqrt(mean_squared_error(y_test_a, y_pred_a))\n",
    "test_mae_a = mean_absolute_error(y_test_a, y_pred_a)\n",
    "\n",
    "print(\"\\n--- TEST SET PERFORMANCE (Model A) ---\")\n",
    "print(f\"Test R²: {test_r2_a:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_a:.4f}\")\n",
    "print(f\"Test MAE: {test_mae_a:.4f}\")\n",
    "\n",
    "# Compare train vs test\n",
    "print(f\"\\nTrain R²: {train_r2_a:.4f}\")\n",
    "print(f\"Test R²: {test_r2_a:.4f}\")\n",
    "print(f\"Difference: {train_r2_a - test_r2_a:.4f}\")\n",
    "if train_r2_a - test_r2_a > 0.1:\n",
    "    print(\"⚠️ Large gap suggests possible overfitting\")\n",
    "else:\n",
    "    print(\"✓ Model generalizes well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: FEATURE IMPORTANCE (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot feature importance\ncoef_df_a_sorted = coef_df_a.sort_values('Coefficient')\ncolors = ['green' if x > 0 else 'red' for x in coef_df_a_sorted['Coefficient']]\n\nplt.figure(figsize=(10, 6))\nplt.barh(coef_df_a_sorted['Feature'], coef_df_a_sorted['Coefficient'], color=colors, edgecolor='black')\nplt.xlabel('Standardized Coefficient', fontsize=12)\nplt.title('Feature Importance: Model A (Land Deals → Climate Cases) - MINIMAL', fontsize=14, fontweight='bold')\nplt.axvline(0, color='black', linestyle='--', linewidth=0.8)\nplt.tight_layout()\nplt.savefig('results/minimal/modelA_feature_importance_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: INTERACTION TERM (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction: Total_Deal_Size × Corruption_Score\n",
    "X_a_scaled['Deal_x_Corruption'] = X_a_scaled['Total_Deal_Size'] * X_a_scaled['Corruption_Score']\n",
    "\n",
    "# Re-split with interaction\n",
    "X_train_a_int, X_test_a_int, y_train_a_int, y_test_a_int = train_test_split(\n",
    "    X_a_scaled, y_a, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model with interaction\n",
    "model_a_int = LinearRegression()\n",
    "model_a_int.fit(X_train_a_int, y_train_a_int)\n",
    "\n",
    "# Get interaction coefficient\n",
    "interaction_coef = model_a_int.coef_[-1]\n",
    "train_r2_a_int = model_a_int.score(X_train_a_int, y_train_a_int)\n",
    "\n",
    "print(\"\\n--- MODEL A WITH INTERACTION ---\")\n",
    "print(f\"Interaction coefficient (Deal_x_Corruption): {interaction_coef:.4f}\")\n",
    "print(f\"\\nTrain R² (without interaction): {train_r2_a:.4f}\")\n",
    "print(f\"Train R² (with interaction): {train_r2_a_int:.4f}\")\n",
    "print(f\"Change in R²: {train_r2_a_int - train_r2_a:.4f}\")\n",
    "\n",
    "# Test statistical significance (simplified - using change in R²)\n",
    "if train_r2_a_int - train_r2_a > 0.01:\n",
    "    print(\"\\n✓ Interaction appears meaningful (ΔR² > 0.01)\")\n",
    "else:\n",
    "    print(\"\\n✗ Interaction does not improve model substantially\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: RESIDUAL DIAGNOSTICS (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate residuals\ny_pred_train_a = model_a.predict(X_train_a)\nresiduals_a = y_train_a - y_pred_train_a\n\n# Create diagnostic plots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Residuals vs Fitted\naxes[0].scatter(y_pred_train_a, residuals_a, alpha=0.6, edgecolors='k')\naxes[0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Fitted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residuals vs Fitted Values')\n\n# Plot 2: Histogram of residuals\naxes[1].hist(residuals_a, bins=20, edgecolor='black', alpha=0.7)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Histogram of Residuals')\n\n# Plot 3: Q-Q plot\nstats.probplot(residuals_a, dist=\"norm\", plot=axes[2])\naxes[2].set_title('Q-Q Plot')\n\nplt.tight_layout()\nplt.savefig('results/minimal/modelA_residual_diagnostics_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n--- Residual Diagnostics ---\")\nprint(\"Check plots for:\")\nprint(\"- Residuals vs Fitted: Should show no pattern (random scatter around 0)\")\nprint(\"- Histogram: Should be roughly normal\")\nprint(\"- Q-Q Plot: Points should follow diagonal line\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: kNN COMPARISON (Model A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit kNN models with different k values\n",
    "k_values = [3, 5, 7]\n",
    "knn_results_a = {}\n",
    "\n",
    "print(\"\\n--- kNN Comparison (Model A) ---\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train_a, y_train_a)\n",
    "    y_pred_knn = knn.predict(X_test_a)\n",
    "    rmse_knn = np.sqrt(mean_squared_error(y_test_a, y_pred_knn))\n",
    "    r2_knn = r2_score(y_test_a, y_pred_knn)\n",
    "    knn_results_a[k] = {'RMSE': rmse_knn, 'R2': r2_knn}\n",
    "    print(f\"k={k}: Test RMSE = {rmse_knn:.4f}, Test R² = {r2_knn:.4f}\")\n",
    "\n",
    "# Find best k\n",
    "best_k_a = min(knn_results_a, key=lambda k: knn_results_a[k]['RMSE'])\n",
    "print(f\"\\nBest k: {best_k_a} (RMSE = {knn_results_a[best_k_a]['RMSE']:.4f})\")\n",
    "\n",
    "# Compare to linear regression\n",
    "print(f\"\\nLinear Regression Test RMSE: {test_rmse_a:.4f}\")\n",
    "print(f\"Best kNN (k={best_k_a}) Test RMSE: {knn_results_a[best_k_a]['RMSE']:.4f}\")\n",
    "\n",
    "if test_rmse_a < knn_results_a[best_k_a]['RMSE']:\n",
    "    print(\"\\n✓ Linear Regression performs better\")\n",
    "else:\n",
    "    print(f\"\\n✓ kNN (k={best_k_a}) performs better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model A Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save model summary\nwith open('results/minimal/modelA_summary_minimal.txt', 'w') as f:\n    f.write(\"MODEL A: Land Deals → Climate Cases (MINIMAL - 5 predictors)\\n\")\n    f.write(\"=\" * 50 + \"\\n\\n\")\n    f.write(f\"Sample Size: {len(df_a_clean)} countries\\n\")\n    f.write(f\"Train/Test Split: {len(X_train_a)}/{len(X_test_a)}\\n\")\n    f.write(f\"Observations per predictor: {len(df_a_clean) / len(predictors_a):.1f}\\n\\n\")\n    f.write(\"COEFFICIENTS:\\n\")\n    f.write(coef_df_a.to_string(index=False))\n    f.write(f\"\\n\\nIntercept: {model_a.intercept_:.4f}\\n\\n\")\n    f.write(\"PERFORMANCE:\\n\")\n    f.write(f\"Train R²: {train_r2_a:.4f}\\n\")\n    f.write(f\"Test R²: {test_r2_a:.4f}\\n\")\n    f.write(f\"Test RMSE: {test_rmse_a:.4f}\\n\")\n    f.write(f\"Test MAE: {test_mae_a:.4f}\\n\\n\")\n    f.write(\"INTERACTION TERM:\\n\")\n    f.write(f\"Deal_x_Corruption Coefficient: {interaction_coef:.4f}\\n\")\n    f.write(f\"R² with interaction: {train_r2_a_int:.4f}\\n\")\n    f.write(f\"Change in R²: {train_r2_a_int - train_r2_a:.4f}\\n\")\n\nprint(\"\\nSaved: results/minimal/modelA_summary_minimal.txt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# MODEL B: Land Deals → ISDS Cases (All Sectors - REVISED)\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: DATA INSPECTION (Model B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter countries with ISDS cases (REVISED: using ALL sectors, not just Ag/Mining)\ndf_b = merged[merged['ISDS_Cases'] > 0].copy()\n\nprint(f\"Countries with ISDS cases (all sectors): {len(df_b)}\")\nprint(f\"\\nShape: {df_b.shape}\")\n\n# Missing values\nprint(\"\\n--- Missing Values ---\")\nprint(df_b.isnull().sum()[df_b.isnull().sum() > 0])\n\n# Descriptive statistics\nprint(\"\\n--- Descriptive Statistics ---\")\ncols_of_interest_b = ['ISDS_Cases', 'Total_Deal_Size', 'Num_Deals', 'Corruption_Score', 'Press_Freedom_Score']\nprint(df_b[cols_of_interest_b].describe())\n\n# Check DV distribution\nprint(\"\\n--- DV Distribution (ISDS_Cases - All Sectors) ---\")\nprint(f\"Skewness: {df_b['ISDS_Cases'].skew():.2f}\")\nprint(f\"Mean: {df_b['ISDS_Cases'].mean():.2f}, Median: {df_b['ISDS_Cases'].median():.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot DV distribution (MINIMAL: using all ISDS cases)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Original\naxes[0].hist(df_b['ISDS_Cases'], bins=20, edgecolor='black')\naxes[0].set_title('Original: ISDS Cases (All Sectors)')\naxes[0].set_xlabel('ISDS Cases')\naxes[0].set_ylabel('Frequency')\n\n# Log-transformed\ndf_b['ISDS_Cases_Log'] = np.log(df_b['ISDS_Cases'] + 1)\naxes[1].hist(df_b['ISDS_Cases_Log'], bins=20, edgecolor='black', color='orange')\naxes[1].set_title('Log-Transformed: log(ISDS Cases + 1)')\naxes[1].set_xlabel('log(ISDS Cases + 1)')\naxes[1].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.savefig('results/minimal/modelB_dv_distribution_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 2-9: Full Pipeline (Model B - REVISED)\n\nRunning the same pipeline as Model A, but with ALL ISDS cases as the outcome (not just Ag/Mining sectors)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STEP 2: CORRELATION CHECK (MINIMAL: 5 predictors)\npredictors_b = ['Total_Deal_Size', 'Num_Deals',\n                'Corruption_Score', 'Press_Freedom_Score',\n                'log_Population']\n\ndf_b_clean = df_b.dropna(subset=predictors_b)\nprint(f\"After dropping missing: {len(df_b_clean)} countries\")\nprint(f\"Observations per predictor: {len(df_b_clean) / len(predictors_b):.1f}\\n\")\n\ncorr_matrix_b = df_b_clean[predictors_b].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix_b, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Matrix - Model B Predictors (MINIMAL)', fontsize=14)\nplt.tight_layout()\nplt.savefig('results/minimal/modelB_correlation_matrix_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# STEP 3: PREPARE DATA\ny_b = df_b_clean['ISDS_Cases_Log']\nX_b = df_b_clean[predictors_b]\n\nscaler_b = StandardScaler()\nX_b_scaled = pd.DataFrame(scaler_b.fit_transform(X_b), columns=X_b.columns, index=X_b.index)\n\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n    X_b_scaled, y_b, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set: {len(X_train_b)}, Test set: {len(X_test_b)}\")\n\n# STEP 4: FIT MODEL\nmodel_b = LinearRegression()\nmodel_b.fit(X_train_b, y_train_b)\n\ncoef_df_b = pd.DataFrame({\n    'Feature': X_train_b.columns,\n    'Coefficient': model_b.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\n--- MODEL B: Land Deals → ISDS Cases (All Sectors - MINIMAL) ---\")\nprint(coef_df_b.to_string(index=False))\n\ntrain_r2_b = model_b.score(X_train_b, y_train_b)\nprint(f\"\\nTrain R²: {train_r2_b:.4f}\")\n\n# STEP 5: TEST EVALUATION\ny_pred_b = model_b.predict(X_test_b)\ntest_r2_b = r2_score(y_test_b, y_pred_b)\ntest_rmse_b = np.sqrt(mean_squared_error(y_test_b, y_pred_b))\ntest_mae_b = mean_absolute_error(y_test_b, y_pred_b)\n\nprint(f\"\\nTest R²: {test_r2_b:.4f}\")\nprint(f\"Test RMSE: {test_rmse_b:.4f}\")\nprint(f\"Test MAE: {test_mae_b:.4f}\")\n\n# STEP 6: FEATURE IMPORTANCE\ncoef_df_b_sorted = coef_df_b.sort_values('Coefficient')\ncolors_b = ['green' if x > 0 else 'red' for x in coef_df_b_sorted['Coefficient']]\n\nplt.figure(figsize=(10, 6))\nplt.barh(coef_df_b_sorted['Feature'], coef_df_b_sorted['Coefficient'], color=colors_b, edgecolor='black')\nplt.xlabel('Standardized Coefficient', fontsize=12)\nplt.title('Feature Importance: Model B (ISDS Cases - All Sectors) - MINIMAL', fontsize=14, fontweight='bold')\nplt.axvline(0, color='black', linestyle='--', linewidth=0.8)\nplt.tight_layout()\nplt.savefig('results/minimal/modelB_feature_importance_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# STEP 7: INTERACTION\nX_b_scaled['Deal_x_Corruption'] = X_b_scaled['Total_Deal_Size'] * X_b_scaled['Corruption_Score']\nX_train_b_int, X_test_b_int, y_train_b_int, y_test_b_int = train_test_split(\n    X_b_scaled, y_b, test_size=0.2, random_state=42\n)\n\nmodel_b_int = LinearRegression()\nmodel_b_int.fit(X_train_b_int, y_train_b_int)\ntrain_r2_b_int = model_b_int.score(X_train_b_int, y_train_b_int)\n\nprint(f\"\\nInteraction coefficient: {model_b_int.coef_[-1]:.4f}\")\nprint(f\"R² change: {train_r2_b_int - train_r2_b:.4f}\")\n\n# STEP 8: RESIDUALS\ny_pred_train_b = model_b.predict(X_train_b)\nresiduals_b = y_train_b - y_pred_train_b\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\naxes[0].scatter(y_pred_train_b, residuals_b, alpha=0.6, edgecolors='k')\naxes[0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Fitted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residuals vs Fitted Values')\n\naxes[1].hist(residuals_b, bins=15, edgecolor='black', alpha=0.7)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Histogram of Residuals')\n\nstats.probplot(residuals_b, dist=\"norm\", plot=axes[2])\naxes[2].set_title('Q-Q Plot')\n\nplt.tight_layout()\nplt.savefig('results/minimal/modelB_residual_diagnostics_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# STEP 9: kNN\nknn_results_b = {}\nprint(\"\\n--- kNN Comparison (Model B) ---\")\nfor k in [3, 5, 7]:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train_b, y_train_b)\n    y_pred_knn = knn.predict(X_test_b)\n    rmse_knn = np.sqrt(mean_squared_error(y_test_b, y_pred_knn))\n    knn_results_b[k] = rmse_knn\n    print(f\"k={k}: RMSE = {rmse_knn:.4f}\")\n\nbest_k_b = min(knn_results_b, key=knn_results_b.get)\nprint(f\"\\nBest kNN: k={best_k_b} (RMSE={knn_results_b[best_k_b]:.4f})\")\nprint(f\"Linear Regression RMSE: {test_rmse_b:.4f}\")\n\n# Save summary\nwith open('results/minimal/modelB_summary_minimal.txt', 'w') as f:\n    f.write(\"MODEL B: Land Deals → ISDS Cases (All Sectors - MINIMAL - 5 predictors)\\n\")\n    f.write(\"=\" * 50 + \"\\n\\n\")\n    f.write(f\"Sample Size: {len(df_b_clean)} countries\\n\")\n    f.write(f\"Observations per predictor: {len(df_b_clean) / len(predictors_b):.1f}\\n\\n\")\n    f.write(coef_df_b.to_string(index=False))\n    f.write(f\"\\n\\nTrain R²: {train_r2_b:.4f}\\n\")\n    f.write(f\"Test R²: {test_r2_b:.4f}\\n\")\n    f.write(f\"Test RMSE: {test_rmse_b:.4f}\\n\")\n\nprint(\"\\nSaved: results/minimal/modelB_summary_minimal.txt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# COMPARATIVE ANALYSIS: Model A vs Model B\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comparison table (MINIMAL)\ncomparison = pd.DataFrame({\n    'Dataset': ['Model A: Climate Cases', 'Model B: ISDS Cases (All Sectors)'],\n    'N': [len(df_a_clean), len(df_b_clean)],\n    'Predictors': [len(predictors_a), len(predictors_b)],\n    'Obs/Predictor': [len(df_a_clean)/len(predictors_a), len(df_b_clean)/len(predictors_b)],\n    'Train R²': [train_r2_a, train_r2_b],\n    'Test R²': [test_r2_a, test_r2_b],\n    'Test RMSE': [test_rmse_a, test_rmse_b],\n    'Best kNN k': [best_k_a, best_k_b],\n    'Best kNN RMSE': [knn_results_a[best_k_a]['RMSE'], knn_results_b[best_k_b]],\n    'Best Model': ['Linear Reg' if test_rmse_a < knn_results_a[best_k_a]['RMSE'] else f'kNN (k={best_k_a})',\n                   'Linear Reg' if test_rmse_b < knn_results_b[best_k_b] else f'kNN (k={best_k_b})']\n})\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINAL COMPARISON: Model A vs Model B (MINIMAL - 5 PREDICTORS)\")\nprint(\"=\" * 80)\nprint(comparison.to_string(index=False))\n\n# Save comparison\ncomparison.to_csv('results/minimal/model_comparison_minimal.csv', index=False)\nprint(\"\\nSaved: results/minimal/model_comparison_minimal.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Coefficient comparison plot (MINIMAL)\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Combine coefficients\ncoef_comparison = pd.merge(\n    coef_df_a[['Feature', 'Coefficient']].rename(columns={'Coefficient': 'Climate Cases'}),\n    coef_df_b[['Feature', 'Coefficient']].rename(columns={'Coefficient': 'ISDS Cases'}),\n    on='Feature'\n)\n\nx = np.arange(len(coef_comparison))\nwidth = 0.35\n\nax.barh(x - width/2, coef_comparison['Climate Cases'], width, label='Model A: Climate', color='steelblue', edgecolor='black')\nax.barh(x + width/2, coef_comparison['ISDS Cases'], width, label='Model B: ISDS (All Sectors)', color='coral', edgecolor='black')\n\nax.set_yticks(x)\nax.set_yticklabels(coef_comparison['Feature'])\nax.set_xlabel('Standardized Coefficient', fontsize=12)\nax.set_title('Coefficient Comparison: Climate vs ISDS (MINIMAL - 5 Predictors)', fontsize=14, fontweight='bold')\nax.axvline(0, color='black', linestyle='--', linewidth=0.8)\nax.legend()\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('results/minimal/coefficient_comparison_minimal.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"KEY FINDINGS (MINIMAL - 5 PREDICTORS)\")\nprint(\"=\" * 80)\n\nprint(\"\\n1. MODEL PERFORMANCE:\")\nprint(f\"   - Climate Cases: Test R² = {test_r2_a:.3f}, RMSE = {test_rmse_a:.3f}\")\nprint(f\"   - ISDS Cases: Test R² = {test_r2_b:.3f}, RMSE = {test_rmse_b:.3f}\")\n\nif test_r2_a > test_r2_b:\n    print(\"   → Land deals BETTER predict climate litigation than ISDS cases\")\nelse:\n    print(\"   → Land deals BETTER predict ISDS cases than climate litigation\")\n\nprint(\"\\n2. TOP PREDICTORS:\")\nprint(\"   Model A (Climate):\")\nprint(f\"   - Most positive: {coef_df_a.iloc[0]['Feature']} ({coef_df_a.iloc[0]['Coefficient']:.3f})\")\nprint(f\"   - Most negative: {coef_df_a.iloc[-1]['Feature']} ({coef_df_a.iloc[-1]['Coefficient']:.3f})\")\n\nprint(\"\\n   Model B (ISDS - All Sectors):\")\nprint(f\"   - Most positive: {coef_df_b.iloc[0]['Feature']} ({coef_df_b.iloc[0]['Coefficient']:.3f})\")\nprint(f\"   - Most negative: {coef_df_b.iloc[-1]['Feature']} ({coef_df_b.iloc[-1]['Coefficient']:.3f})\")\n\nprint(\"\\n3. INTERACTION EFFECTS:\")\nprint(f\"   - Model A (Deal × Corruption): ΔR² = {train_r2_a_int - train_r2_a:.4f}\")\nprint(f\"   - Model B (Deal × Corruption): ΔR² = {train_r2_b_int - train_r2_b:.4f}\")\n\nprint(\"\\n4. MINIMAL PREDICTOR SET (5 variables):\")\nprint(\"   Removed (poor coverage):\")\nprint(\"     ✗ Rule_of_Law_Score_Pct\")\nprint(\"     ✗ Avg_Deal_Size (multicollinear)\")\nprint(\"     ✗ Prop_Agriculture (52% missing in Model A)\")\nprint(\"     ✗ Literacy_Rate_Pct (26% missing in Model A)\")\nprint(\"\\n   Kept (core theory + control):\")\nprint(\"     ✓ Total_Deal_Size\")\nprint(\"     ✓ Num_Deals\")\nprint(\"     ✓ Corruption_Score\")\nprint(\"     ✓ Press_Freedom_Score\")\nprint(\"     ✓ log_Population\")\n\nprint(\"\\n5. SAMPLE SIZE IMPROVEMENTS:\")\nprint(f\"   Model A: {len(df_a_clean)} countries (was 23 with 7 predictors)\")\nprint(f\"   Model B: {len(df_b_clean)} countries (was 72 with 7 predictors)\")\nprint(f\"   Observations per predictor:\")\nprint(f\"     - Model A: {len(df_a_clean)/len(predictors_a):.1f} (was 3.3)\")\nprint(f\"     - Model B: {len(df_b_clean)/len(predictors_b):.1f} (was 10.3)\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nMinimal analysis complete! Check results/minimal/ folder for all outputs.\")\nprint(\"For the 7-predictor version, see: RQ2_Land_Deals_to_Litigation_Analysis_REVISED.ipynb\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}